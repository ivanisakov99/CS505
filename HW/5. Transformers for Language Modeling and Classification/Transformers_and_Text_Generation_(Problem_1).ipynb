{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljhQ3WC79pAa"
      },
      "source": [
        "# From: Transformers and Text-Generation\n",
        "by Liam Dugan (UPenn). \n",
        "\n",
        "\n",
        "Please write your answers and code in the cells with questions below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwQJB7Ft9wJ3"
      },
      "source": [
        "----------\n",
        "\n",
        "For this homework, we will take ideas from the entire class: language models, text generation, vector-based word representations, syntactic analysis, and neural networks. We'll be using large, pre-trained language models to generate text, and studying how we can fine-tune these large language models to generate text in whatever genre and style we want!\n",
        "\n",
        "In this assignment you will get:\n",
        "1. An overview of the \"Transformer\" architecture is and why it is particularly well suited for Natural Language Processing tasks\n",
        "2. An introduction to the Generative Pretrained Transformer (GPT) family, which is a set of large-scale language models that can be used to generate text that often sounds like it was written by a human.\n",
        "3. Experience with using the HuggingFace package to fine-tune these models to generate text that sounds like it comes from a specific source."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLt2-8LK9pO_"
      },
      "source": [
        "# Part 1: What is a Transformer? (Reading)\n",
        "<figure align=\"center\">\n",
        "<img src=\"https://media.giphy.com/media/VeWllmR9zfaco/giphy.gif\" />\n",
        "<figcaption>(It's probably not this guy, right?)</figcaption>\n",
        "</figure>\n",
        "\n",
        "### The Transformer\n",
        "\n",
        "The current state-of-the-art for a variety of natural language processing tasks belongs to the **Transformer** architecture, first published December 6th 2017. \n",
        "\n",
        "The Transformer can be thought of as a big feed-forward network with every feed-forward layer containing something called an \"attention module\". \n",
        "\n",
        ">You might be wondering: why are we moving back to feed-forward networks after having so much success with recurrent neural networks and variants like LSTMs? Aren't RNNs naturally poised to handle sequences as their inputs? Well, as it turns out, the sequential nature of RNNs make them really difficult to train in a distributed/parallel fashion. So while RNNs make more sense to use on sequences of inputs, serial networks such as the transformer can be trained much faster, allowing orders of magnitude more training data to be used. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kDBzHuT-vPj"
      },
      "source": [
        "\n",
        "### Reading \\# 1 - What is a Transformer?\n",
        "\n",
        "In order to get a good grasp on exactly *why* these models are so good it's important to understand what they are and how they work. \n",
        "\n",
        "Your first task for this homework is to read the blog post [\"The Illustrated Transformer\" by Jay Alammar](http://jalammar.github.io/illustrated-transformer/). This blog post explains the transformer architecture (and the all-important \"Attention Module\") with helpful visualizations and diagrams. \n",
        "\n",
        "**You should read this post very closely and understand exactly what the Transformer is and how it works. Once you're finished reading, answer the following questions in 2-3 sentences each.**\n",
        "\n",
        "1. (2 pts) What is Self-Attention (at a high level)?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "2. (2 pts) How is Self-Attention computed?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "3. (2 pts) What do the \"Query\", \"Key\", and \"Value\" vectors encode (at a high level)?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "4. (2 pts) What is an attention \"head\" and why should we use multiple heads?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "5. (2 pts) What are positional embeddings?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "6. (2 pts) Why are positional embeddings important?\n",
        "\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZ-OTakPCPJc"
      },
      "source": [
        "-----------\n",
        "### Reading \\# 2 - Transformer Language Models\n",
        "\n",
        "On June 11th 2018, OpenAI released a model named **GPT**, standing for *Generative Pre-Trained Transformer*. This model was a Transformer architecture that was modified such that it could be used for Text Generation instead of sequence to sequence modeling. This model was also pre-trained, which means that anyone could download the fully trained model and use it without needing to train the model themselves. \n",
        "\n",
        "On February 14th 2019, OpenAI released a blog post detailing a brand new version of GPT that had an insane **1.5 billion parameters**. They named this version **GPT-2**. To train such a large model, OpenAI crawled 40GB worth of text from the web (roughly 20,000,000,000 words). \n",
        "\n",
        "GPT-2 is an extremely impressive language model that can generate text that often sounds so plausible that it seems like it might have been written by a human.  Here is an example of what GPT-2 can generate, taken from [OpenAI’s blog post](https://openai.com/blog/better-language-models/):\n",
        "\n",
        "**Human-Written Prompt:**\n",
        "> “Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.”\n",
        "\n",
        "**Model Continuation:**\n",
        "> The 19-year-old singer was caught on camera being escorted out of the store by security guards.\n",
        "> \n",
        "> The singer was wearing a black hoodie with the label ‘Blurred Lines’ on the front and ‘Fashion Police’ on the back.\n",
        "> \n",
        "> Scroll down for video\n",
        "> \n",
        "> Shoplifting: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today (pictured)\n",
        "> \n",
        "> The singer was also wearing a pair of black-rimmed glasses, a black jacket, black jeans and black sandals.\n",
        "> \n",
        "> She was carrying a pair of black and white striped gloves and a small black bag.\n",
        "\n",
        "*(To be clear: Miley Cyrus was never actually arrested for shoplifting)*.  \n",
        "\n",
        "The quality of the model's output was so good, that the researchers at OpenAI were worried that, in addition to positive applications, it might be used for malicious purposes like generating fake news, impersonating people online, creating abusive or fake content to post on social media, or automating spam and phishing scams. In a [demo to Wired Magazine](https://www.wired.com/story/ai-text-generator-too-dangerous-to-make-public/), the reporter typed in “Hillary Clinton and George Soros” and GPT-2 generated a crazy rant with lots of conspiracy theories: \n",
        "> Hillary Clinton and George Soros are a perfect match, and their agenda appears to be to create a political movement where Soros and his political machine and Clinton are two of the only major players. This is the first time Soros and Clinton have been caught on tape directly colluding in promoting the same false narrative. One of the key revelations in the leaked audio was Clinton's admission to a Russian banker that she knew about the Uranium One deal before it was approved by Congress. Clinton was shown sharing the same talking points that were originally drafted by a Fusion GPS contractor hired by an anti-Trump Republican donor. The leaked audio is the clearest evidence yet that the Clinton campaign and the Hillary Foundation colluded with Fusion GPS to manufacture propaganda against President Trump.\n",
        "\n",
        "\n",
        "They were concerned enough that they labeled GPT-2 \"too dangerous to release\", and OpenAI initially refused to release their dataset, training code, or GPT-2 model weights.  OpenAI decided to release in a delayed, phased fashion so that researchers could spend time working on automatic detection of generated text.\n",
        "\n",
        "In this homework, you'll get to be the judge of how good GPT-2 is, as you'll be using it yourself to generate text!\n",
        "\n",
        "**To start your journey into the world of Text Generation, you should read Part 1 of the blog post [\"The Illustrated GPT-2\" by Jay Alammar](http://jalammar.github.io/illustrated-gpt2/) and answer the following questions in 2-3 sentences each**\n",
        "\n",
        "7. (4 pts) How does the architecture of GPT-2 differ from the standard Encoder-Decoder Transformer model?\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "8. (4 pts) What is the difference between \"Masked Self-Attention\" and \"Self-Attention\"\n",
        "   > YOUR ANSWER HERE\n",
        "   \n",
        "9. (4 pts) What are logits? How are they computed? and How does GPT-2 use them to decide which word to predict next?\n",
        "   > YOUR ANSWER HERE\n",
        "\n",
        "### Aside: GPT-3 \n",
        "\n",
        "On June 11th 2020, OpenAI released GPT-3 [(paper)](https://arxiv.org/pdf/2005.14165.pdf) [(wikipedia)](https://en.wikipedia.org/wiki/GPT-3). This model has an unfathomable **175 billion parameters** (100x larger than GPT-2!) and was trained on 570GB of text! This model is virtually indistinguishable from human output and can generate text about any topic and in any style with only a few words of priming text. It can do some very terrifying things.\n",
        "\n",
        "GPT-3 Can:\n",
        "- Generate JSX code off natural language descriptions\n",
        "- Generate Emojis based off of descriptions of the feeling\n",
        "- Generate regular expressions off natural language descriptions\n",
        "- Generate website mockups off natural language descriptions\n",
        "- Generate charts with titles, labels and legends from natural language descriptions\n",
        "- Explain python code in plain english\n",
        "- Automatically generate quiz questions (and grade them)\n",
        "- Generate Latex from natural language descriptions\n",
        "- Generate Linux commands from natural language descriptions\n",
        "- Generate a Machine Learning model from natural language descriptions\n",
        "\n",
        "[Here's a collection of 21 things GPT-3 can do (with examples)](https://machinelearningknowledge.ai/openai-gpt-3-demos-to-convince-you-that-ai-threat-is-real-or-is-it/#OpenAI_GPT-3_Demos)\n",
        "\n",
        "[Here's a NYT article about how GPT-3 can write code, poetry, and argue](https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html)\n",
        "\n",
        "[Here's an article GPT-3 wrote for The Guardian about how it loves humans and would never subjugate humanity](https://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3)\n",
        "\n",
        "**You may optionally choose to read Jay Alammar's most recent blog post [\"How GPT3 Works - Visualizations and Animations\"](http://jalammar.github.io/how-gpt3-works-visualizations-animations/) from July 2020 if you're curious as to how GPT-3 differs from GPT-2**\n",
        "\n",
        "Similarly to GPT-2, OpenAI has decided not to release GPT-3, this time opting to put GPT-3 behind an API which you need to request permission to use. This allows them to control exactly who can generate text and what type of text is generated. While this is a good solution in the short term, the long term implications of GPT-3 are still unclear.\n",
        "\n",
        "If you are interested in trying out GPT-3 yourself, feel free to [Join the OpenAI API Waitlist](https://share.hsforms.com/1Lfc7WtPLRk2ppXhPjcYY-A4sk30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFqk7jra-Li"
      },
      "source": [
        "-------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y14gbpSjOTt7"
      },
      "source": [
        "# Part 2: GPT-2 Text Generation with HuggingFace\n",
        "\n",
        "Phew, that was a lot of reading. Now lets get to the fun part! Let's use the transformer to generate some text!!\n",
        "\n",
        "We will use the [Transformers library from HuggingFace](https://transformer.huggingface.co), which provides support for many Transformer-based language models like GPT-2. \n",
        "\n",
        "**IMPORTANT: Make sure that you have GPU set as your Hardware Accelerator in `Runtime > Change runtime type` before running this Colab.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6yWI0ae9knK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLdW3FOcIRqi"
      },
      "source": [
        "## 2.1 The 'Pipeline' Interface\n",
        "\n",
        "The simplest way to use the HuggingFace library is to use their [Pipeline interface](https://huggingface.co/transformers/main_classes/pipelines.html)\n",
        "\n",
        "There are many different types of Pipelines available but in this section we'll use the TextGenerationPipeline to get up and running with pretrained gpt2 as fast as possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpOwY1ZVOew4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MCgVfPSG8Sg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Note: device=0 means to use GPU, device=-1 is to use CPU\n",
        "generator = pipeline('text-generation', model='gpt2', device=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_gc4VRmHCHA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "outputs = generator('I wonder what I will generate?')\n",
        "print(outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWDdGag_UrkA"
      },
      "source": [
        "Note that the 'text-generation' pipeline will work with any **auto-regressive** language model (a.k.a 'causal-lm' models according to the HuggingFace lingo). You can find a list of all such models here https://huggingface.co/models?filter=causal-lm. \n",
        "\n",
        "10. (6 pts) **Your first task is to use the Pipeline interface to get generation output below for at least two different 'causal-lm' models (One of these two can be a different version of GPT2, but make sure at least one is a non-gpt family language model)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYbWdguUU2Bl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE FOR MODEL 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZLHYzr4U54s",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE FOR MODEL 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joicqRgHJHcW"
      },
      "source": [
        "## 2.2 Dissecting the Pipeline\n",
        "Now that was easy!\n",
        "\n",
        "As beautiful and easy as the Pipeline interface is, we want to know what's going on under the hood!\n",
        "\n",
        "There are four main steps to a text generation pipeline:\n",
        "1. (Tokenize) Turn the raw input text into a vector of integer token IDs using a tokenizer\n",
        "\n",
        "2. (Encode) Feed those token IDs into the language model by querying for each token's embedding in the model's embedding matrix (the \"encoder\") and then feed the \"encoded\" sequence into the decoder module\n",
        "\n",
        "3. (Decode) The decoder will output logits (a probability distribution over all possible integer token IDs) and we sample from those logits to get our next token -- repeat until EOS token is generated or we hit max_length\n",
        "\n",
        "4. (Detokenize) Take the output sequence of token IDs and turn them from integer token IDs back to tokens with the tokenizer\n",
        "\n",
        "Below you'll see how HuggingFace does this:\n",
        "\n",
        "First we have to initialize both the tokenizer and the model from their pre-trained checkpoints. Note that the tokenizer has to match the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRgGzAuTJakH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel# AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azhDlBgYM9bP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#### Step 1: Tokenize the input into integer token IDs\n",
        "inputs = tokenizer.encode(\"Hello, how are you?\", return_tensors='pt').to(model.device)\n",
        "print(\"Input Token IDs: \" + str(inputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaFR6j-5Rz6C",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#### Step 2 and 3: Feed in the integer token IDs and get out a sequence of token IDs as output\n",
        "outputs = model.generate(inputs)\n",
        "print(\"Output Token IDs: \" + str(outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tO1NWK9DSD8u",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#### Step 4: Feed in the integer token IDs and get out a sequence of token IDs as output\n",
        "output_text = [tokenizer.decode(x) for x in outputs]\n",
        "print(\"Output Text: \" + str(output_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNRN-CfvOiU0"
      },
      "source": [
        "Now that you have dissected the pipeline, it's time to play with some common parameters!\n",
        "\n",
        "[Check out this demo notebook from HuggingFace](https://github.com/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb) for a good overview of the different generation parameters and what they do (with example code!).\n",
        "\n",
        "The full documentation on all of the parameters you can use in the generate function can be found [here](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate)\n",
        "\n",
        "As an example, below we have a call to generate that:\n",
        "- randomly samples from the top 50 words in the output distribution (rather than just greedily picking the best one every time)\n",
        "- downweights the probability of all previously generated tokens by a factor of 1.2 (to prevent repetition)\n",
        "- goes on for 512 tokens, because its more interesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fNJ5InvTtgm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer.encode(\"Hello, how are you?\", return_tensors='pt').to(model.device)\n",
        "outputs = model.generate(\n",
        "      inputs,\n",
        "      do_sample=True,          # Randomly sample from the logits instead of greedily picking next word with highest probability\n",
        "      top_k=50,                 # Only sample from the top 50 most likely words\n",
        "      repetition_penalty=1.2,    # Downweights the probability of all previously generated tokens by a factor of 1.2\n",
        "      max_length=512          # Generate for a maximum of 512 tokens\n",
        "  )\n",
        "print([tokenizer.decode(x) for x in outputs][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSb6SAgnV4GD"
      },
      "source": [
        "**11. Your job is to provide two different examples of generation output from GPT-2 with different choices of generation parameters. You must also provide a 1-2 sentence explanation of what these parameters do and how they affect your output**\n",
        "\n",
        "Feel free to get creative with this! Really poke around and try to find the combination of settings that gives you the best sounding text! The ways in which these parameters affect how 'human-like' a section of generated text sounds is an area of active research. :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3VcRifdWRNf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE FOR HYPERPARAMETER VARIATION 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-4Xj-9mXNNF"
      },
      "source": [
        "(4 pts) YOUR ANSWER HERE - EXPLANATION FOR HPARAM VARIATION 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FosxFhqoWUsG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE FOR HYPERPARAMETER VARIATION 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8G2LIiIWX5R"
      },
      "source": [
        "(4 pts) YOUR ANSWER HERE -- EXPLANATION FOR HPARAM VARIATION 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3rz6Nd2XUXc"
      },
      "source": [
        "## 2.3 Fine-Tuning GPT-2\n",
        "Okay now time for the best part!\n",
        "\n",
        "Generating general-purpose text from pre-trained models is great, but what if we want our text to be in a specific genre or style? Luckily for us, the GPT family of models use the idea of \"Transfer learning\" -- using knowledge gained from one problem (or training setting), and applying it to another area or domain. The idea of transfer learning for NLP, is that we can train a language model on general texts, and then adapt it to use it for a specific task or domain that we're interested in. This process is also called **fine-tuning**.\n",
        "\n",
        "In this section we'll walk you through an example of using HuggingFace to fine-tune GPT-2 and then you'll be asked to fine-tune GPT-2 on two datasets of your own choosing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtTdNBlOckGX"
      },
      "source": [
        "### Fine-Tuning Example using HuggingFace Datasets library: Crime and Punishment\n",
        "\n",
        "For our fine-tuning example we're going to train GPT-2 to mimic the style of Fyodor Dostoevsky's novel \"Crime and Punishment\"\n",
        "\n",
        "We will be downloading our data using the HuggingFace [Datasets](https://huggingface.co/docs/datasets/) library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGUNYK5M4iMa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsHEIKHS2_4O",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "import datasets\n",
        "from datasets import load_dataset, list_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyTmMnOVc_ab"
      },
      "source": [
        "### Step 1: Initialize a Brand New GPT-2 Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0JbWSYvQydJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2').cuda()\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVMuJpwLdGWI"
      },
      "source": [
        "### Step 2: Load the text of \"Crime and Punishment\" and tokenize it\n",
        "\n",
        "The 'load_dataset' function queries for a dataset with a certain tag and downloads the corresponding data from HuggingFace's hosting site. This allows us to download all sorts of datasets through the same interface!\n",
        "\n",
        "The documentation for load_dataset can be found [here](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset)\n",
        "\n",
        "Here we take our tokenizer and run it on the entirety of Crime and Punishment in a single batch by using map on our custom encode function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82TWhrUBHbGs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def encode(batch): return tokenizer([x.strip('\\n\\r') for x in batch['line']], truncation=True, padding=True)\n",
        "\n",
        "crime_and_punishment = load_dataset('crime_and_punish', split='train')\n",
        "processed = crime_and_punishment.map(encode, batched=True, batch_size=len(crime_and_punishment))\n",
        "processed.set_format('torch', columns=['input_ids', 'attention_mask'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tq-2h_RQeYqe"
      },
      "source": [
        "### Step 3: Initialize the Trainer\n",
        "\n",
        "The 'Trainer' module is the main way we perform fine-tuning. In order to initialize a Trainer, you need a model, tokenizer, TrainingArguments, your training data (in a Dataset object) and something called a data_collator (which tells the Trainer not to look for a vector of labels). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xYPXcIPADUB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='/content/',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    logging_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=processed,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zrom-2JlfvET"
      },
      "source": [
        "### Step 4: Fine-Tune the Model!\n",
        "\n",
        "Now we're done! All we have to do is hit run and sit back!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui52Yu-pA9c3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ_D6i-Wf4eV"
      },
      "source": [
        "### Step 5: Save the Model and use it to Generate!\n",
        "\n",
        "Save your fine-tuned model and compare its output with regular GPT-2's output to see the difference for yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhRSQdbsS3fo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "trainer.save_model('./dostoevskypt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vI8OQBmwTLF3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dostoevskypt2 = pipeline('text-generation', model='./dostoevskypt2', device=0)\n",
        "gpt2 = pipeline('text-generation', model='gpt2', device=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwH-idynVTvl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(dostoevskypt2('Saint Petersburg is'))\n",
        "print(gpt2('Saint Petersburg is'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAtTbv1MSMoQ"
      },
      "source": [
        "## PERPLEXITY\n",
        "\n",
        "12. (2 pts) Using the pointer [here](https://huggingface.co/transformers/perplexity.html), compute the perplexity of the GPT2 pre-trained model on the Wikipedia test set (you can keep the same hyperparameters as in the link) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5NMY7SFbsTg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON WIKIPEDIA TEST SET \n",
        "\n",
        "# ANSWERS BELOW:\n",
        "# Load wiki test set\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")\n",
        "max_length = model.config.n_positions\n",
        "stride = 512\n",
        "\n",
        "# Define a function for ppl\n",
        "def ppl(model, input_ids_all, stride):\n",
        "  nlls = []\n",
        "  for i in tqdm(range(0, input_ids_all.size(1), stride)):\n",
        "      begin_loc = max(i + stride - max_length, 0)\n",
        "      end_loc = min(i + stride, input_ids_all.size(1))\n",
        "      trg_len = end_loc - i  # may be different from stride on last loop\n",
        "      input_ids = input_ids_all[:, begin_loc:end_loc].to(\"cuda:0\")\n",
        "      target_ids = input_ids.clone()\n",
        "      target_ids[:, :-trg_len] = -100\n",
        "\n",
        "      with torch.no_grad():\n",
        "          outputs = model(input_ids, labels=target_ids)\n",
        "          neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "      nlls.append(neg_log_likelihood)\n",
        "\n",
        "  ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
        "  return ppl\n",
        "ppl(model, encodings.input_ids, stride)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uIEdUD-isKa"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE: 87.48 (-20, +10 are fine, beyond that give partial credit, deducting 0.5 as it gets worse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbogHdnDqxEc"
      },
      "source": [
        "13. (2 pts) Compute the  perplexity of the dostoevskypt2 model on Wikipedia test set\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtw3vYOebx7Z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF DOSTOEVSKYPT2 ON WIKIPEDIA TEST SET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2oad0unX9ts"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87eu1LZjTGrs"
      },
      "source": [
        "14. (2 pts) Compute the perplexity of the GPT2 pre-trained model on the Crime and Punishment train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-E7mkr3nb1xg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON CRIME AND PUNISHMENT TRAIN DATASET "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5iaw2ztixEk"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrQR4yoPTT4K"
      },
      "source": [
        "15. (2 pts) Compute the **train** perplexity of the **dostoevskypt2** model \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHpNOEIXb6Zd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF DOSTOEVSKYPT2 ON CRIME AND PUNISHMENT TRAIN DATASET "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajI9qA0WizXA"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mKQvTJUUv-e"
      },
      "source": [
        "> (1 pt) Which model performs better on Crime and Punishment train set, vanilla GPT-2 or your dostoevskypt2 checkpoint?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26FMGt734liX"
      },
      "source": [
        "> YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF3fs8BOcYKv"
      },
      "source": [
        "16. (2 pts) Compute perplexity of the GPT2 model on your raw pride and prejudice text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1de_sUUgcnyp",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF GPT2 ON PRIDE AND PREJUDICE TEXT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mb6kYP86cpav"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5NtBjuicsmF"
      },
      "source": [
        "17. (2 pts) Compute perplexity of the **dostoevskypt2** model on your raw pride and prejudice text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIc124pqdDZJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR COMPUTING PERPLEXITY OF dostoevskipt2 ON PRIDE AND PREJUDICE TEXT "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qTNzx8IdCmr"
      },
      "source": [
        "> YOUR PERPLEXITY ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vff5qb5BSq6j"
      },
      "source": [
        "### Now's Your Turn!\n",
        "\n",
        "**Your job is to fine-tune GPT2 one more time with your choice of fine-tuning dataset.**\n",
        "\n",
        "*****For the fine-tuned model you create, you should clearly demonstrate (through visible generation outputs and analysis) that your fine-tuned model follows the desired style better than vanilla GPT2** ***\n",
        "\n",
        "Please make sure to give a brief description \n",
        "\n",
        "In order to see which datasets are available for download, run the cell below. Pick one that you think would be interesting!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kclua3zSqNz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "datasets_list = list_datasets()\n",
        "print(', '.join(dataset for dataset in datasets_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6NsclQPi4XI"
      },
      "source": [
        "### Tips\n",
        "- Most of the datasets hosted by HuggingFace are not meant for Causal LM fine-tuning. Make sure you preprocess them accordingly if you want to use them.\n",
        "- In order to check out information about a dataset hosted by huggingface you can use [this web viewer](https://huggingface.co/datasets/viewer/?dataset=crime_and_punish). Try to avoid downloading a dataset that's too big!\n",
        "- You will likely have to change the custom 'encode' function for each new dataset you want to fine-tune on. You need to change batch['line'] to instead index with the correct column label for your specific dataset (it probably wont be called 'line').\n",
        "\n",
        "### Useful Links\n",
        "[load_datasets Documentation](https://huggingface.co/docs/datasets/package_reference/loading_methods.html#datasets.load_dataset)\n",
        "\n",
        "[Trainer Documentation](https://huggingface.co/transformers/main_classes/trainer.html#id1)\n",
        "\n",
        "[Example: Fine-Tuning BERT for Esperanto](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=zTgWPa9Dipk2)\n",
        "\n",
        "[Example: Fine-Tuning for IMDb Classification](https://colab.research.google.com/drive/1-JIJlao4dI-Ilww_NnTc0rxtp-ymgDgM?usp=sharing#scrollTo=5DEWNilys9Ty)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzjWvQnft8t2"
      },
      "source": [
        "#### 18. Dataset \\#1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3M0lPs5mMkk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR FINE-TUNING GPT2 ON DATASET "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6NwNFURsQe_"
      },
      "source": [
        "(4 pts) YOUR ANSWER HERE - BRIEF DESCRIPTION OF THE DATASET YOU CHOSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWJ6yNlysx79",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "## YOUR CODE HERE - FOR GENERATION WITH YOUR FINE-TUNED MODEL AND COMPARISON WITH REGULAR GPT2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXLqnbnhs_nC"
      },
      "source": [
        "(5 pts) YOUR ANSWER HERE - COMPARISON OF YOUR DATASET'S FINE-TUNED OUTPUT VS NON-FINE-TUNED OUTPUT \n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Spring22 Homework 5-Transformers and Text-Generation (Problem 1)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
