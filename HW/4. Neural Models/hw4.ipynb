{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bbf7a8",
   "metadata": {
    "id": "7cWykkppMxLh",
    "papermill": {
     "duration": 0.015651,
     "end_time": "2022-08-20T18:09:46.582325",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.566674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Assignment 4 (100 pts, BONUS: 10 pts) - Neural Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00248f3b",
   "metadata": {
    "id": "aHMJpjYZMxLj",
    "papermill": {
     "duration": 0.013146,
     "end_time": "2022-08-20T18:09:46.608953",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.595807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this assignment, you will learn about text classification and language modeling using RNN and LSTM, and use **pytorch** — a deep learning framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36ab4a8",
   "metadata": {
    "id": "sXI02-lFMxLj",
    "papermill": {
     "duration": 0.012943,
     "end_time": "2022-08-20T18:09:46.636209",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.623266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem 1. Neural Models (35 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0af67a",
   "metadata": {
    "id": "QvoGgVj2MxLk",
    "papermill": {
     "duration": 0.012975,
     "end_time": "2022-08-20T18:09:46.662869",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.649894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. (2 pts) In a news framing classification task, where you have 5 frames and your model predicts each of the frames with equal probability for an article, what is the cross entropy loss of the article in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8195f65",
   "metadata": {
    "id": "7luvMAhTMxLk",
    "papermill": {
     "duration": 0.012996,
     "end_time": "2022-08-20T18:09:46.689205",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.676209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6988e7",
   "metadata": {
    "id": "qrt6msauMxLk",
    "papermill": {
     "duration": 0.013058,
     "end_time": "2022-08-20T18:09:46.715479",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.702421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. (2 pts) Suppose during training of your neural model you realize that your training loss remains high. Mention some of the ways you can reduce this **underfitting** of your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bf8ee7",
   "metadata": {
    "id": "2knIhNfhMxLk",
    "papermill": {
     "duration": 0.013028,
     "end_time": "2022-08-20T18:09:46.741807",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.728779",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db140313",
   "metadata": {
    "id": "QdDIvQRxMxLk",
    "papermill": {
     "duration": 0.013015,
     "end_time": "2022-08-20T18:09:46.767905",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.754890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. (2 pts) After you do many changes to your neural network, you now realize that your training loss is much lower than your validation loss. Mention some of the ways you can reduce this **overfitting** of your neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb36a205",
   "metadata": {
    "id": "d9GgwRbSMxLk",
    "papermill": {
     "duration": 0.012938,
     "end_time": "2022-08-20T18:09:46.794039",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.781101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a03472a",
   "metadata": {
    "id": "mJ4W26YRMxLl",
    "papermill": {
     "duration": 0.012933,
     "end_time": "2022-08-20T18:09:46.820076",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.807143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. (2 pts) What is good about setting a large batch size for training? How about a small batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bf4ba",
   "metadata": {
    "id": "Bwfc3yfdMxLl",
    "papermill": {
     "duration": 0.013394,
     "end_time": "2022-08-20T18:09:46.846575",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.833181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f04f5eb",
   "metadata": {
    "id": "IAGxxqzFMxLl",
    "papermill": {
     "duration": 0.013733,
     "end_time": "2022-08-20T18:09:46.873844",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.860111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. (4 pts) How can an RNN be used for detecting toxic spans (spans of words containing toxic language) in a social media comment? Specifically, what should be the input to the RNN at each time step t? How many outputs (i.e., $\\hat{y}$) are produced given a comment containing $n$ words? What is each $\\hat{y}^{(t)}$ a probability distribution over?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2e03bb",
   "metadata": {
    "id": "qpWBGSTFMxLl",
    "papermill": {
     "duration": 0.012998,
     "end_time": "2022-08-20T18:09:46.899958",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.886960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17423f85",
   "metadata": {
    "id": "OaeVrcH_MxLl",
    "papermill": {
     "duration": 0.013051,
     "end_time": "2022-08-20T18:09:46.926291",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.913240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. (4 pts) How about using RNNs for language modeling? Given a start word token as input at time step 1, what should be the input to the RNN at each time step t > 1? How many outputs are produced? What is each $\\hat{y}^{(t)}$ a probability distribution over?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f8113d",
   "metadata": {
    "id": "k9NZ3naSMxLm",
    "papermill": {
     "duration": 0.012974,
     "end_time": "2022-08-20T18:09:46.952993",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.940019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b212f428",
   "metadata": {
    "id": "BPYTamm9MxLm",
    "papermill": {
     "duration": 0.012953,
     "end_time": "2022-08-20T18:09:46.979448",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.966495",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. (3 pts) How about using RNNs for frame classification? Given an article containing $n$ words as input, what should be the input to the RNN at each time step t? How many outputs are produced? What is each $\\hat{y}^{(t)}$ a probability distribution over?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24be56a7",
   "metadata": {
    "id": "Hj3an3olMxLm",
    "papermill": {
     "duration": 0.013296,
     "end_time": "2022-08-20T18:09:47.006611",
     "exception": false,
     "start_time": "2022-08-20T18:09:46.993315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3438863",
   "metadata": {
    "id": "8GOZID7cMxLm",
    "papermill": {
     "duration": 0.0134,
     "end_time": "2022-08-20T18:09:47.033182",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.019782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8. (2 pts) What is the main advantage of using RNNs for frame classification over feed forward neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65d598b",
   "metadata": {
    "id": "OBhhX6DjMxLm",
    "papermill": {
     "duration": 0.013587,
     "end_time": "2022-08-20T18:09:47.059959",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.046372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc50a2",
   "metadata": {
    "id": "-V4LeMEoMxLm",
    "papermill": {
     "duration": 0.013157,
     "end_time": "2022-08-20T18:09:47.086272",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.073115",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 9. (4 pts) What is the disadvantage of RNN when used to classify the sentiment of a very long tweet like this? “I am not sure I want this phone. It’s too big to fit in my back pocket. I put it in and accidentally sat on it and now it’s bent. I’m very disappointed. I’m now the proud owner of bendy iPhone13. Very proud.” What is the appropriate sentiment for this tweet? And what would the RNN classify it as?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf11d1f",
   "metadata": {
    "id": "CC_UY4OMMxLn",
    "papermill": {
     "duration": 0.012989,
     "end_time": "2022-08-20T18:09:47.112446",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.099457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb22140",
   "metadata": {
    "id": "3Sh-8Cw9MxLn",
    "papermill": {
     "duration": 0.013116,
     "end_time": "2022-08-20T18:09:47.138679",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.125563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 10. How about LSTM? Given this formulation of LSTM: $ f_t = \\sigma(W_fx_t + U_fh_{t - 1} + b_f) \\text{ (forget gate)}, i_t = \\sigma(W_ix_t + U_ih_{t - 1} + b_i) \\text{ and } \\hat{C}_t = \\tanh(W_Cx_t + U_Ch_{t - 1} + b_C) \\text{ (input gate)}, C_t = f_t * C_{t - 1} + i_t * \\hat{C}_t \\text{ (update gate), and } \\omicron_t = \\sigma(W_{\\omicron}x_t + U_{\\omicron}h_{t - 1} + b_{\\omicron}) \\text{ and } h_t = \\omicron_t * \\tanh(C_t) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec68d4dd",
   "metadata": {
    "id": "dS-vCM0GMxLn",
    "papermill": {
     "duration": 0.013031,
     "end_time": "2022-08-20T18:09:47.165789",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.152758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(a) (4 pts) Derive the formulation of $\\frac{\\partial{J}}{U_C}$ , where J is the loss function, for two time steps $t$ and $t − 1$ in terms of $\\frac{\\partial{J}}{\\partial{h_t}}$, $\\frac{\\partial{h_t}}{\\partial{C_t}}$, $\\frac{\\partial{C_t}}{\\partial{U_C}}$, $\\frac{\\partial{C_t}}{\\partial{C_{t - 1}}}$, $\\frac{\\partial{C_{t - 1}}}{\\partial{U_C}}$, $\\frac{\\partial{h_t}}{\\partial{h_{t - 1}}}$, and $\\frac{\\partial{h_{t - 1}}}{\\partial{U_C}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227a207",
   "metadata": {
    "id": "TYCY0YZ0MxLn",
    "papermill": {
     "duration": 0.013005,
     "end_time": "2022-08-20T18:09:47.191933",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.178928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4958d1e2",
   "metadata": {
    "id": "SSMyIPQDMxLn",
    "papermill": {
     "duration": 0.012906,
     "end_time": "2022-08-20T18:09:47.218264",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.205358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(b) (2 pts) Which part of $\\frac{\\partial{J}}{U_C}$ reduces the effect of the vanishing gradient problem in RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e489c8",
   "metadata": {
    "id": "qcNg4He7MxLn",
    "papermill": {
     "duration": 0.013296,
     "end_time": "2022-08-20T18:09:47.244685",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.231389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12149446",
   "metadata": {
    "id": "gT_URHdyMxLo",
    "papermill": {
     "duration": 0.01297,
     "end_time": "2022-08-20T18:09:47.271343",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.258373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(c)  (2 pts) How does this help classify the correct sentiment of the tweet above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13927e51",
   "metadata": {
    "id": "kGrHANfEMxLo",
    "papermill": {
     "duration": 0.012964,
     "end_time": "2022-08-20T18:09:47.297410",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.284446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a5aa3b",
   "metadata": {
    "id": "j0OsMhusMxLo",
    "papermill": {
     "duration": 0.013377,
     "end_time": "2022-08-20T18:09:47.323942",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.310565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "(d) (2 pts) Instead of using the last hidden state of LSTM to classify the tweet, what other ways we can do to improve the performance of this sentiment classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d33ac6",
   "metadata": {
    "id": "YA51MT-eMxLo",
    "papermill": {
     "duration": 0.013024,
     "end_time": "2022-08-20T18:09:47.350100",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.337076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "-> answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd74f9",
   "metadata": {
    "id": "wqxbnivEMxLo",
    "papermill": {
     "duration": 0.012995,
     "end_time": "2022-08-20T18:09:47.377039",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.364044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem 2. LSTM for language modeling (33 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be5bea0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:09:47.405382Z",
     "iopub.status.busy": "2022-08-20T18:09:47.404847Z",
     "iopub.status.idle": "2022-08-20T18:09:51.423475Z",
     "shell.execute_reply": "2022-08-20T18:09:51.422229Z"
    },
    "id": "jbVRBQ08MxLo",
    "outputId": "dd82ed60-c038-4367-f728-bd6b11b74e1d",
    "papermill": {
     "duration": 4.037567,
     "end_time": "2022-08-20T18:09:51.427769",
     "exception": false,
     "start_time": "2022-08-20T18:09:47.390202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "\n",
    "path = '/kaggle/input/cs505-hw4-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398c87de",
   "metadata": {
    "id": "bB55TFKWMxLr",
    "papermill": {
     "duration": 0.014952,
     "end_time": "2022-08-20T18:09:51.457920",
     "exception": false,
     "start_time": "2022-08-20T18:09:51.442968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. (10 pts) Follow the tutorial in [here](https://www.analyticsvidhya.com/blog/2020/08/build-a-natural-language-generation-nlg-system-using-pytorch/) to train a word-level LSTM language modeling. Train the language model on texts from the file prideAndPrejudice.txt. Before using it to train the language model, you need to first sentence-segment, then tokenize, then lower case each line of the file using Spacy. Append start-of-sentence token $\\text{'<s>'}$ and end-of-sentence $\\text{'</s>'}$ token to each **sentence** and put each sentence in its own line. Use only words that appear more than once in this corpus and assign UNK tokens for the rest; you may also need to pad sentences that are shorter than 5 (see [here](https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb) in cell 10-12 for adding unknown: UNK token and padding: PAD token to your vocabulary). Train the language model and save the trained model (see [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)). Generate 10 examples of text from it, starting from $\\text{'<s>'}$ token and ending at $\\text{'</s>'}$ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ff30087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:09:51.490579Z",
     "iopub.status.busy": "2022-08-20T18:09:51.490090Z",
     "iopub.status.idle": "2022-08-20T18:09:52.638508Z",
     "shell.execute_reply": "2022-08-20T18:09:52.637554Z"
    },
    "id": "LcFygNSTMxLr",
    "papermill": {
     "duration": 1.167138,
     "end_time": "2022-08-20T18:09:52.640943",
     "exception": false,
     "start_time": "2022-08-20T18:09:51.473805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "with open(path + 'prideAndPrejudice.txt') as f:\n",
    "    for line in f:\n",
    "        text.append(line.strip())\n",
    "\n",
    "text = ' '.join(text)\n",
    "text_tokens = []\n",
    "for sentence in nltk.sent_tokenize(text):\n",
    "    text_tokens.append(['<s>'] + nltk.word_tokenize(sentence) + ['</s>'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d197e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:09:52.669733Z",
     "iopub.status.busy": "2022-08-20T18:09:52.669416Z",
     "iopub.status.idle": "2022-08-20T18:09:55.548360Z",
     "shell.execute_reply": "2022-08-20T18:09:55.547366Z"
    },
    "id": "cWBfHh-2MxLr",
    "papermill": {
     "duration": 2.895639,
     "end_time": "2022-08-20T18:09:55.550554",
     "exception": false,
     "start_time": "2022-08-20T18:09:52.654915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dict(text_tokens, appear_time=1):\n",
    "    vocab = Counter(sum(text_tokens, []))\n",
    "\n",
    "    # Removing the words that only appear once\n",
    "    vocab = {k: v for k, v in vocab.items() if v > appear_time}\n",
    "\n",
    "    # Sorting the words according to the number of appearances, with the most common word being first\n",
    "    vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "\n",
    "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "    vocab = ['<pad>', '<unk>'] + vocab\n",
    "\n",
    "    # Dictionaries to store the word to index mappings and vice versa\n",
    "    word2idx = {o: i for i, o in enumerate(vocab)}\n",
    "    idx2word = {i: o for i, o in enumerate(vocab)}\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "\n",
    "word2idx, idx2word = get_dict(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "258b260d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:09:55.580850Z",
     "iopub.status.busy": "2022-08-20T18:09:55.580508Z",
     "iopub.status.idle": "2022-08-20T18:09:55.603280Z",
     "shell.execute_reply": "2022-08-20T18:09:55.602400Z"
    },
    "id": "wjIrC8OtMxLt",
    "papermill": {
     "duration": 0.040667,
     "end_time": "2022-08-20T18:09:55.605219",
     "exception": false,
     "start_time": "2022-08-20T18:09:55.564552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def createSequences(tokens, sequenceLength=5):\n",
    "    sequences = []\n",
    "    if len(tokens) > sequenceLength:\n",
    "        for i in range(0, len(tokens) - sequenceLength):\n",
    "            # select sequence of tokens\n",
    "            sequence = tokens[i:i + sequenceLength]\n",
    "            # add to the list\n",
    "            sequences.append(' '.join(sequence))\n",
    "\n",
    "        return sequences\n",
    "    else:\n",
    "        sequence = tokens[:]\n",
    "        # pad sequence to 5\n",
    "        for i in range(len(tokens), sequenceLength):\n",
    "            sequence.append('<pad>')\n",
    "        return [' '.join(sequence)]\n",
    "\n",
    "\n",
    "def get_integer_seq(sequence):\n",
    "    return [\n",
    "        word2idx[w] if w in word2idx.keys() else word2idx['<unk>']\n",
    "        for w in sequence.split()\n",
    "    ]\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, text_tokens, sequenceLength=5):\n",
    "        seqs = [\n",
    "            createSequences(tokens, sequenceLength) for tokens in text_tokens\n",
    "        ]\n",
    "\n",
    "        # merge list-of-lists into a single list\n",
    "        seqs = sum(seqs, [])\n",
    "\n",
    "        # create inputs and targets (x and y)\n",
    "        x, y = [], []\n",
    "\n",
    "        for s in seqs:\n",
    "            x.append(' '.join(s.split()[:-1]))\n",
    "            y.append(' '.join(s.split()[1:]))\n",
    "\n",
    "        # convert text sequences to integer sequences\n",
    "        x = [get_integer_seq(i) for i in x]\n",
    "        y = [get_integer_seq(i) for i in y]\n",
    "\n",
    "        # convert lists to numpy arrays\n",
    "        self.x = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class WordLSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size=len(word2idx),\n",
    "                 embed_dim=200,\n",
    "                 n_hidden=256,\n",
    "                 n_layers=4,\n",
    "                 drop_prob=0.3):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.emb_layer = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        ## define the LSTM\n",
    "        self.lstm = nn.LSTM(embed_dim,\n",
    "                            n_hidden,\n",
    "                            n_layers,\n",
    "                            dropout=drop_prob,\n",
    "                            batch_first=True)\n",
    "\n",
    "        ## define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "\n",
    "        ## define the fully-connected layer\n",
    "        self.fc = nn.Linear(n_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "\n",
    "        ## pass input through embedding layer\n",
    "        embedded = self.emb_layer(x)\n",
    "\n",
    "        ## Get the outputs and the new hidden state from the lstm\n",
    "        if hidden != None:\n",
    "            lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        else:\n",
    "            lstm_output, hidden = self.lstm(embedded)\n",
    "\n",
    "        ## pass through a dropout layer\n",
    "        out = self.dropout(lstm_output)\n",
    "\n",
    "        #out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = out.reshape(-1, self.n_hidden)\n",
    "\n",
    "        ## put \"out\" through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (weight.new(self.n_layers, batch_size,\n",
    "                             self.n_hidden).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size,\n",
    "                             self.n_hidden).zero_().to(device))\n",
    "\n",
    "        return hidden\n",
    "\n",
    "\n",
    "def train(network, data, epochs=5, clip=1, interval=800, sequenceLength=5):\n",
    "\n",
    "    network = network.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    loss_function = loss_function.to(device)\n",
    "    optimiser = torch.optim.Adam(network.parameters(), lr=1e-4)\n",
    "\n",
    "    network.train()\n",
    "    min_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = []\n",
    "        _ = network.init_hidden(batch_size)\n",
    "\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            inputs, targets = x.long().to(device), y.long().to(device)\n",
    "            output, _ = network(inputs, None)\n",
    "            loss = loss_function(output, targets.view(-1))\n",
    "\n",
    "            network.zero_grad()\n",
    "            loss.backward()\n",
    "            epoch_loss.append(loss.item())\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "            optimiser.step()\n",
    "\n",
    "            if i % interval == 0:\n",
    "                print('Epoch: {}/{}, {}%, Loss: {}'.format(\n",
    "                    epoch + 1, epochs, round(i / len(data) * 100, 2), loss),\n",
    "                      end='\\r')\n",
    "\n",
    "        mean_epoch_loss = np.mean(epoch_loss)\n",
    "        print('Epoch: {}/{}, Train Loss: {}'.format(epoch + 1, epochs,\n",
    "                                                    mean_epoch_loss))\n",
    "\n",
    "        if mean_epoch_loss < min_loss:\n",
    "            if not os.path.exists('./models'):\n",
    "                os.mkdir('./models')\n",
    "            torch.save(network.state_dict(),\n",
    "                       './models/bestModel_' + str(sequenceLength) + '.pt')\n",
    "            print('Best model saved\\n')\n",
    "            min_loss = mean_epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875a8c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:09:55.634087Z",
     "iopub.status.busy": "2022-08-20T18:09:55.633814Z",
     "iopub.status.idle": "2022-08-20T18:11:50.309394Z",
     "shell.execute_reply": "2022-08-20T18:11:50.308403Z"
    },
    "id": "OCC6mDgOMxLu",
    "outputId": "c74b70e3-6da6-42c3-bdb5-906f35fd4279",
    "papermill": {
     "duration": 114.692989,
     "end_time": "2022-08-20T18:11:50.311666",
     "exception": false,
     "start_time": "2022-08-20T18:09:55.618677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "WordLSTM(\n",
      "  (emb_layer): Embedding(4152, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=4152, bias=True)\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Train Loss: 6.065429639391079\n",
      "Best model saved\n",
      "\n",
      "Epoch: 2/5, Train Loss: 5.610063239419536\n",
      "Best model saved\n",
      "\n",
      "Epoch: 3/5, Train Loss: 5.2835640301066595\n",
      "Best model saved\n",
      "\n",
      "Epoch: 4/5, Train Loss: 5.067543165000381\n",
      "Best model saved\n",
      "\n",
      "Epoch: 5/5, Train Loss: 4.9195671284426545\n",
      "Best model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_data_5 = DataLoader(SequenceDataset(text_tokens),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "network = WordLSTM()\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network, data=train_data_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6664f35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:11:50.345046Z",
     "iopub.status.busy": "2022-08-20T18:11:50.343386Z",
     "iopub.status.idle": "2022-08-20T18:11:50.353999Z",
     "shell.execute_reply": "2022-08-20T18:11:50.353045Z"
    },
    "id": "gh36StEDMxLu",
    "papermill": {
     "duration": 0.028788,
     "end_time": "2022-08-20T18:11:50.355948",
     "exception": false,
     "start_time": "2022-08-20T18:11:50.327160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    value, _ = torch.topk(logits, k)\n",
    "    logits[logits < value[:, [-1]]] = -float('Inf')\n",
    "    return logits\n",
    "\n",
    "\n",
    "# predict next token\n",
    "@torch.no_grad()\n",
    "def predict(network, word, hidden, top_k, sample=True):\n",
    "    word = word if word in word2idx else '<unk>'\n",
    "\n",
    "    # tensor inputs\n",
    "    x = np.array([[word2idx[word]]])\n",
    "    inputs = torch.tensor(x).to(device)\n",
    "\n",
    "    # detach hidden state from history\n",
    "    hidden = tuple([i.data for i in hidden])\n",
    "\n",
    "    # get the output of the network\n",
    "    logits, hidden = network(inputs, hidden)\n",
    "\n",
    "    # get the token probabilities\n",
    "    logits = top_k_logits(logits, top_k)\n",
    "\n",
    "    # apply softmax to convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # sample from the distribution or take the most likely\n",
    "    idx = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "    # return the encoded value of the predicted word and the hidden state\n",
    "    return idx2word[idx.item()], hidden\n",
    "\n",
    "\n",
    "# function to generate text\n",
    "def sample(network, max_step=30, context='<s>', top_k=20, sample=True):\n",
    "    # push to GPU\n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    # batch size is 1\n",
    "    hidden = network.init_hidden(1)\n",
    "    tokens = context.split()\n",
    "\n",
    "    # predict subsequent tokens\n",
    "    for _ in range(max_step):\n",
    "        token, hidden = predict(network, tokens[-1], hidden, top_k)\n",
    "        tokens.append(token)\n",
    "        if token == '<\\s>':\n",
    "            return ' '.join(tokens)\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5dbe884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:11:50.387357Z",
     "iopub.status.busy": "2022-08-20T18:11:50.386401Z",
     "iopub.status.idle": "2022-08-20T18:11:50.667428Z",
     "shell.execute_reply": "2022-08-20T18:11:50.666557Z"
    },
    "id": "urLB-w0XMxLv",
    "papermill": {
     "duration": 0.299218,
     "end_time": "2022-08-20T18:11:50.669825",
     "exception": false,
     "start_time": "2022-08-20T18:11:50.370607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_sent = []\n",
    "for _ in range(10):\n",
    "    generate_sent.append(sample(network))\n",
    "\n",
    "if not os.path.exists('./sentences'):\n",
    "    os.mkdir('./sentences')\n",
    "\n",
    "with open('./sentences/generate_sent_1.txt', 'w') as f:\n",
    "    for sent in generate_sent:\n",
    "        f.write('{}\\n'.format(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46348df7",
   "metadata": {
    "id": "MjzLutg6MxLv",
    "papermill": {
     "duration": 0.033569,
     "end_time": "2022-08-20T18:11:50.742342",
     "exception": false,
     "start_time": "2022-08-20T18:11:50.708773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. (7 pts) Compute and report the perplexity of the saved model on *test_1.txt* file. Note that the test files are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851cd7b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:11:50.816320Z",
     "iopub.status.busy": "2022-08-20T18:11:50.815968Z",
     "iopub.status.idle": "2022-08-20T18:11:50.829450Z",
     "shell.execute_reply": "2022-08-20T18:11:50.828395Z"
    },
    "id": "p9Uyp1LbMxLv",
    "papermill": {
     "duration": 0.059172,
     "end_time": "2022-08-20T18:11:50.833310",
     "exception": false,
     "start_time": "2022-08-20T18:11:50.774138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prob(network, word, hidden, target):\n",
    "    word = word if word in word2idx else '<unk>'\n",
    "    target = target if target in word2idx else '<unk>'\n",
    "\n",
    "    x = np.array([[word2idx[word]]])\n",
    "    y = np.array([[word2idx[target]]])\n",
    "\n",
    "    inputs = torch.tensor(x).to(device)\n",
    "    hidden = tuple([i.data for i in hidden])\n",
    "\n",
    "    logits, hidden = network(inputs, hidden)\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    prob = probs[0, y].item()\n",
    "    return prob, hidden\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_perplex(network, test_tokens):\n",
    "    perplexity = []\n",
    "\n",
    "    network.to(device)\n",
    "    network.eval()\n",
    "\n",
    "    for tokens in test_tokens:\n",
    "        sent_perplex = 0\n",
    "        hidden = network.init_hidden(1)\n",
    "\n",
    "        for i in range(len(tokens) - 1):\n",
    "            prob, hidden = get_prob(network, tokens[i], hidden, tokens[i + 1])\n",
    "            sent_perplex += -np.log(prob)\n",
    "\n",
    "        perplexity.append(sent_perplex / len(tokens))\n",
    "\n",
    "    test_perplexity = np.exp(np.mean(perplexity))\n",
    "    print('Testing perplexity: {}'.format(test_perplexity))\n",
    "    return test_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f509a348",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:11:50.892366Z",
     "iopub.status.busy": "2022-08-20T18:11:50.892083Z",
     "iopub.status.idle": "2022-08-20T18:12:04.883968Z",
     "shell.execute_reply": "2022-08-20T18:12:04.882423Z"
    },
    "id": "NkHl3CzCMxLv",
    "outputId": "75255284-6a1e-445c-f776-ffd44721b838",
    "papermill": {
     "duration": 14.02021,
     "end_time": "2022-08-20T18:12:04.886232",
     "exception": false,
     "start_time": "2022-08-20T18:11:50.866022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing perplexity: 140.56896915784304\n"
     ]
    }
   ],
   "source": [
    "seqLengthPerplexity = {}\n",
    "test_tokens = []\n",
    "with open(path + 'test_1.txt') as f:\n",
    "    for line in f:\n",
    "        test_tokens.append(line.strip().split())\n",
    "\n",
    "seqLengthPerplexity['5'] = compute_perplex(network, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e15a4",
   "metadata": {
    "id": "J_rqVtSzMxLv",
    "papermill": {
     "duration": 0.014576,
     "end_time": "2022-08-20T18:12:04.916244",
     "exception": false,
     "start_time": "2022-08-20T18:12:04.901668",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. (5 pts) Train the language model as before, but with input sequence lengths of 25 (currently, it’s inputs are of length 5). You may need to pad some of the shorter sentences to length 25. Save your trained model. Generate 10 examples of text from it, starting from $\\text{'<s>'}$ token and ending at $\\text{'</s>'}$ token. Are there differences from the generated examples from 2.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed609bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:12:04.947570Z",
     "iopub.status.busy": "2022-08-20T18:12:04.946881Z",
     "iopub.status.idle": "2022-08-20T18:13:56.959222Z",
     "shell.execute_reply": "2022-08-20T18:13:56.958278Z"
    },
    "id": "yGDxQmDXMxLv",
    "outputId": "820ecb6a-8f88-4ad6-e971-5f26a97d24cc",
    "papermill": {
     "duration": 112.030326,
     "end_time": "2022-08-20T18:13:56.961460",
     "exception": false,
     "start_time": "2022-08-20T18:12:04.931134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "WordLSTM(\n",
      "  (emb_layer): Embedding(4152, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=4152, bias=True)\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Train Loss: 6.026442472838835\n",
      "Best model saved\n",
      "\n",
      "Epoch: 2/5, Train Loss: 5.869402631903962\n",
      "Best model saved\n",
      "\n",
      "Epoch: 3/5, Train Loss: 5.668060594622424\n",
      "Best model saved\n",
      "\n",
      "Epoch: 4/5, Train Loss: 5.214953619522063\n",
      "Best model saved\n",
      "\n",
      "Epoch: 5/5, Train Loss: 4.964131011390816\n",
      "Best model saved\n",
      "\n",
      "\n",
      "\u001b[1mGenerating Sentences...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_data_25 = DataLoader(SequenceDataset(text_tokens, sequenceLength=25),\n",
    "                           batch_size=batch_size,\n",
    "                           shuffle=True)\n",
    "\n",
    "network = WordLSTM()\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network, data=train_data_25, sequenceLength=25)\n",
    "\n",
    "print('\\n\\033[1mGenerating Sentences...\\033[0m')\n",
    "generate_sent = []\n",
    "for _ in range(10):\n",
    "    generate_sent.append(sample(network))\n",
    "\n",
    "with open('./sentences/generate_sent_2.txt', 'w') as f:\n",
    "    for sent in generate_sent:\n",
    "        f.write('{}\\n'.format(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc22b1",
   "metadata": {
    "id": "Kknxtq25MxLw",
    "papermill": {
     "duration": 0.015462,
     "end_time": "2022-08-20T18:13:56.992971",
     "exception": false,
     "start_time": "2022-08-20T18:13:56.977509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. (2 pts) Compute and report the perplexity of this saved model on *test_1.txt* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558e9677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:13:57.026597Z",
     "iopub.status.busy": "2022-08-20T18:13:57.025096Z",
     "iopub.status.idle": "2022-08-20T18:14:10.953209Z",
     "shell.execute_reply": "2022-08-20T18:14:10.951459Z"
    },
    "id": "EJFbpqJBMxLw",
    "outputId": "eadadf20-2934-4b80-b9f2-ed0fc6da4315",
    "papermill": {
     "duration": 13.947682,
     "end_time": "2022-08-20T18:14:10.956217",
     "exception": false,
     "start_time": "2022-08-20T18:13:57.008535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing perplexity: 138.0627337030298\n"
     ]
    }
   ],
   "source": [
    "seqLengthPerplexity['25'] = compute_perplex(network, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060fcd2e",
   "metadata": {
    "id": "wIWnfj3eMxLw",
    "papermill": {
     "duration": 0.016165,
     "end_time": "2022-08-20T18:14:10.989345",
     "exception": false,
     "start_time": "2022-08-20T18:14:10.973180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. (2 pts) Use the better language model (the one with the lower perplexity on *test_1.txt*) to compute and report the perplexity on *test_2.txt*. Note that the test files are already pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4a658f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:14:11.021678Z",
     "iopub.status.busy": "2022-08-20T18:14:11.021345Z",
     "iopub.status.idle": "2022-08-20T18:14:20.678834Z",
     "shell.execute_reply": "2022-08-20T18:14:20.677594Z"
    },
    "id": "T94Y5dI-MxLw",
    "outputId": "e1b70a25-c479-437f-9f28-38f9ec11d17f",
    "papermill": {
     "duration": 9.676889,
     "end_time": "2022-08-20T18:14:20.681557",
     "exception": false,
     "start_time": "2022-08-20T18:14:11.004668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing perplexity: 226.2459985376151\n"
     ]
    }
   ],
   "source": [
    "test_tokens_2 = []\n",
    "with open(path + 'test_2.txt') as f:\n",
    "    for line in f:\n",
    "        test_tokens_2.append(line.strip().split())\n",
    "\n",
    "network.load_state_dict(\n",
    "    torch.load('./models/bestModel_' +\n",
    "               min(seqLengthPerplexity, key=seqLengthPerplexity.get) + '.pt',\n",
    "               map_location=device))\n",
    "\n",
    "_ = compute_perplex(network, test_tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa434f5",
   "metadata": {
    "id": "9BrZdjVXMxLw",
    "papermill": {
     "duration": 0.015546,
     "end_time": "2022-08-20T18:14:20.713678",
     "exception": false,
     "start_time": "2022-08-20T18:14:20.698132",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. (5 pts) Train the better language model as before but start with pre-trained [Glove6B 100d embeddings](https://nlp.stanford.edu/projects/glove/) (see [here](https://medium.com/@martinpella/how-to-use-pre-trained-word-embeddings-in-pytorch-71ca59249f76) on how to incorporate pretrained embeddings in your LSTM model). This time, use all your words in the corpus as vocabulary, even those occurring only once in the corpus. Only assign UNK token to words that are not in Glove vocabulary and initialize random vectors in the embedding matrix for the UNK, $\\text{'<s>'}$, $\\text{'</s>'}$, and PAD tokens using a standard gaussian distribution with σ set to 0.6 (see [numpy.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html)). Save your trained model. Generate 10 examples of text from it, starting from $\\text{'<s>'}$ token and ending at $\\text{'</s>'}$ token. Are there differences from the generated examples from before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affd795a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:14:20.746904Z",
     "iopub.status.busy": "2022-08-20T18:14:20.745801Z",
     "iopub.status.idle": "2022-08-20T18:17:37.568260Z",
     "shell.execute_reply": "2022-08-20T18:17:37.567237Z"
    },
    "id": "Oi8DrwM5s60q",
    "papermill": {
     "duration": 196.841659,
     "end_time": "2022-08-20T18:17:37.570741",
     "exception": false,
     "start_time": "2022-08-20T18:14:20.729082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./vectorEmbeddings/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
      "100%|█████████▉| 399999/400000 [00:16<00:00, 24394.56it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "Glove = torchtext.vocab.GloVe(name='6B',\n",
    "                              dim=embed_dim,\n",
    "                              cache='./vectorEmbeddings')\n",
    "\n",
    "pretrained_embed = np.zeros((len(word2idx), embed_dim))\n",
    "for i, word in enumerate(word2idx.keys()):\n",
    "    if word in Glove.stoi:\n",
    "        pretrained_embed[i] = Glove[word]\n",
    "    else:\n",
    "        pretrained_embed[i] = np.random.normal(scale=0.6, size=(embed_dim, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f353d18a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:17:37.814702Z",
     "iopub.status.busy": "2022-08-20T18:17:37.814218Z",
     "iopub.status.idle": "2022-08-20T18:19:26.332122Z",
     "shell.execute_reply": "2022-08-20T18:19:26.330598Z"
    },
    "id": "seJ3eCbiMxLx",
    "outputId": "17bddff7-c58c-4750-f617-820e217242f3",
    "papermill": {
     "duration": 108.662638,
     "end_time": "2022-08-20T18:19:26.335217",
     "exception": false,
     "start_time": "2022-08-20T18:17:37.672579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "WordLSTM(\n",
      "  (emb_layer): Embedding(4152, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=4152, bias=True)\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Train Loss: 6.096155865177227\n",
      "Best model saved\n",
      "\n",
      "Epoch: 2/5, Train Loss: 5.78182130971532\n",
      "Best model saved\n",
      "\n",
      "Epoch: 3/5, Train Loss: 5.41578022683502\n",
      "Best model saved\n",
      "\n",
      "Epoch: 4/5, Train Loss: 5.217180793300556\n",
      "Best model saved\n",
      "\n",
      "Epoch: 5/5, Train Loss: 5.0735737828540195\n",
      "Best model saved\n",
      "\n"
     ]
    }
   ],
   "source": [
    "network = WordLSTM(embed_dim=embed_dim)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "network.emb_layer.weight.data.copy_(torch.from_numpy(pretrained_embed))\n",
    "network.emb_layer.weight.requires_grad = True\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network, data=train_data_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79bcb77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:26.598118Z",
     "iopub.status.busy": "2022-08-20T18:19:26.597250Z",
     "iopub.status.idle": "2022-08-20T18:19:26.854940Z",
     "shell.execute_reply": "2022-08-20T18:19:26.853913Z"
    },
    "id": "SfO0Xh73MxLx",
    "papermill": {
     "duration": 0.363523,
     "end_time": "2022-08-20T18:19:26.857470",
     "exception": false,
     "start_time": "2022-08-20T18:19:26.493947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_sent = []\n",
    "for _ in range(10):\n",
    "    generate_sent.append(sample(network))\n",
    "\n",
    "with open('./sentences/generate_sent_3.txt', 'w') as f:\n",
    "    for sent in generate_sent:\n",
    "        f.write('{}\\n'.format(sent))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcecc2",
   "metadata": {
    "id": "FeK2mGTAMxLx",
    "papermill": {
     "duration": 0.096291,
     "end_time": "2022-08-20T18:19:27.051979",
     "exception": false,
     "start_time": "2022-08-20T18:19:26.955688",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. (2 pts) Compute and report the perplexity of this saved model on *test_1.txt* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3e1532b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:27.248159Z",
     "iopub.status.busy": "2022-08-20T18:19:27.247791Z",
     "iopub.status.idle": "2022-08-20T18:19:41.943496Z",
     "shell.execute_reply": "2022-08-20T18:19:41.941323Z"
    },
    "id": "sXunHNB4MxLx",
    "outputId": "22062704-225a-46d5-e00f-0def38ab53ad",
    "papermill": {
     "duration": 14.798066,
     "end_time": "2022-08-20T18:19:41.946551",
     "exception": false,
     "start_time": "2022-08-20T18:19:27.148485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing perplexity: 158.16866387530243\n"
     ]
    }
   ],
   "source": [
    "_ = compute_perplex(network, test_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6723247",
   "metadata": {
    "id": "2aY2PuLAMxLx",
    "papermill": {
     "duration": 0.095052,
     "end_time": "2022-08-20T18:19:42.140054",
     "exception": false,
     "start_time": "2022-08-20T18:19:42.045002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Problem 3. LSTM for classification (32 pts, BONUS: 10 pts)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1445823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:42.331413Z",
     "iopub.status.busy": "2022-08-20T18:19:42.331060Z",
     "iopub.status.idle": "2022-08-20T18:19:42.367064Z",
     "shell.execute_reply": "2022-08-20T18:19:42.365719Z"
    },
    "id": "9-qBU3rPMxLx",
    "outputId": "3d666c8c-e4a4-4220-c235-282b634f74b2",
    "papermill": {
     "duration": 0.134628,
     "end_time": "2022-08-20T18:19:42.369097",
     "exception": false,
     "start_time": "2022-08-20T18:19:42.234469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device:', device)\n",
    "path = '/kaggle/input/cs505-hw4-data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ef68791",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:42.561944Z",
     "iopub.status.busy": "2022-08-20T18:19:42.561576Z",
     "iopub.status.idle": "2022-08-20T18:19:42.729266Z",
     "shell.execute_reply": "2022-08-20T18:19:42.728275Z"
    },
    "id": "o-CpmSHTMxLx",
    "papermill": {
     "duration": 0.266456,
     "end_time": "2022-08-20T18:19:42.731842",
     "exception": false,
     "start_time": "2022-08-20T18:19:42.465386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train, df_test = pd.read_csv(path + 'sentiment-train.csv'), pd.read_csv(\n",
    "    path + 'sentiment-test.csv')\n",
    "\n",
    "X_train, Y_train = df_train['text'].to_list(), df_train['sentiment'].to_list()\n",
    "X_test, Y_test = df_test['text'].to_list(), df_test['sentiment'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c64111",
   "metadata": {
    "id": "WY1fYamSMxLx",
    "papermill": {
     "duration": 0.09566,
     "end_time": "2022-08-20T18:19:42.926458",
     "exception": false,
     "start_time": "2022-08-20T18:19:42.830798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1. (5 pts) Follow the tutorial [here](https://github.com/gabrielloye/LSTM_Sentiment-Analysis/blob/master/main.ipynb) on how to build LSTM model for sentiment classification. Modify the tutorial to train on your tweet sentiment data (sentiment-train.csv) and test on test data (*sentiment-test.csv*) from HW2 (modify the tutorial so that the train data is **not** split into train and validation). Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a4cbb3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:43.120397Z",
     "iopub.status.busy": "2022-08-20T18:19:43.120052Z",
     "iopub.status.idle": "2022-08-20T18:19:43.139203Z",
     "shell.execute_reply": "2022-08-20T18:19:43.138100Z"
    },
    "id": "UD9rPKy5MxLy",
    "papermill": {
     "duration": 0.118732,
     "end_time": "2022-08-20T18:19:43.141765",
     "exception": false,
     "start_time": "2022-08-20T18:19:43.023033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Defining a function that either shortens sentences or pads sentences with 0 to a fixed length\n",
    "def pad_input(sentences, sequenceLength):\n",
    "    features = np.zeros((len(sentences), sequenceLength), dtype=int)\n",
    "    for i, review in enumerate(sentences):\n",
    "        if len(review) > 0:\n",
    "            features[i, :len(review)] = np.array(review)[:sequenceLength]\n",
    "    return features\n",
    "\n",
    "\n",
    "def process(train_sentences,\n",
    "            train_labels,\n",
    "            test_sentences,\n",
    "            test_labels,\n",
    "            padding_length=30):\n",
    "\n",
    "    # Remove any digits\n",
    "    for i in range(len(train_sentences)):\n",
    "        train_sentences[i] = re.sub('\\d', '0', train_sentences[i])\n",
    "    for i in range(len(test_sentences)):\n",
    "        test_sentences[i] = re.sub('\\d', '0', test_sentences[i])\n",
    "\n",
    "    # Modify URLs to <url>\n",
    "    for i in range(len(train_sentences)):\n",
    "        if 'www.' in train_sentences[i] or 'http:' in train_sentences[\n",
    "                i] or 'https:' in train_sentences[\n",
    "                    i] or '.com' in train_sentences[i]:\n",
    "            train_sentences[i] = re.sub(r'([^ ]+(?<=\\.[a-z]{3}))', '<url>',\n",
    "                                        train_sentences[i])\n",
    "    for i in range(len(test_sentences)):\n",
    "        if 'www.' in test_sentences[i] or 'http:' in test_sentences[\n",
    "                i] or 'https:' in test_sentences[\n",
    "                    i] or '.com' in test_sentences[i]:\n",
    "            test_sentences[i] = re.sub(r'([^ ]+(?<=\\.[a-z]{3}))', '<url>',\n",
    "                                       test_sentences[i])\n",
    "\n",
    "    vocab = Counter(\n",
    "    )  # Dictionary that will map a word to the number of times it appeared in all the training sentences\n",
    "    train_tokens = []\n",
    "    for sentence, _ in zip(train_sentences, tqdm(range(len(train_sentences)))):\n",
    "        # The sentences will be stored as a list of words/tokens\n",
    "        tokens = []\n",
    "        for word in nltk.word_tokenize(sentence):  # Tokenizing the words\n",
    "            vocab.update([word.lower()\n",
    "                          ])  # Converting all the words to lower case\n",
    "            tokens.append(word)\n",
    "        train_tokens.append(tokens)\n",
    "\n",
    "    # Removing the words that only appear once\n",
    "    vocab = {k: v for k, v in vocab.items() if v > 1}\n",
    "    # Sorting the words according to the number of appearances, with the most common word being first\n",
    "    vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
    "    # Adding padding and unknown to our vocabulary so that they will be assigned an index\n",
    "    vocab = ['<pad>', '<unk>'] + vocab\n",
    "    # Dictionaries to store the word to index mappings and vice versa\n",
    "    word2idx = {o: i for i, o in enumerate(vocab)}\n",
    "    idx2word = {i: o for i, o in enumerate(vocab)}\n",
    "\n",
    "    train_idx = []\n",
    "    test_idx = []\n",
    "    for i, tokens in enumerate(train_tokens):\n",
    "        # Looking up the mapping dictionary and assigning the index to the respective words\n",
    "        train_idx.append([\n",
    "            word2idx[word] if word in word2idx else word2idx['<unk>']\n",
    "            for word in tokens\n",
    "        ])\n",
    "\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        # For test sentences, we have to tokenize the sentences as well\n",
    "        test_idx.append([\n",
    "            word2idx[word.lower()]\n",
    "            if word.lower() in word2idx else word2idx['<unk>']\n",
    "            for word in nltk.word_tokenize(sentence)\n",
    "        ])\n",
    "\n",
    "    seq_len = padding_length  # The length that the sentences will be padded/shortened to\n",
    "    train_idx = pad_input(train_idx, seq_len)\n",
    "    test_idx = pad_input(test_idx, seq_len)\n",
    "\n",
    "    # Converting our labels into numpy arrays\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_labels = np.array(test_labels)\n",
    "\n",
    "    train_data = TensorDataset(torch.from_numpy(train_idx),\n",
    "                               torch.from_numpy(train_labels))\n",
    "    test_data = TensorDataset(torch.from_numpy(test_idx),\n",
    "                              torch.from_numpy(test_labels))\n",
    "\n",
    "    return word2idx, idx2word, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2439b0aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:43.334636Z",
     "iopub.status.busy": "2022-08-20T18:19:43.333957Z",
     "iopub.status.idle": "2022-08-20T18:19:57.094185Z",
     "shell.execute_reply": "2022-08-20T18:19:57.093189Z"
    },
    "id": "LZXq4jjAMxLy",
    "outputId": "0270a827-6597-4cb7-ebbf-65c200c8d3c1",
    "papermill": {
     "duration": 13.860022,
     "end_time": "2022-08-20T18:19:57.096597",
     "exception": false,
     "start_time": "2022-08-20T18:19:43.236575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 59999/60000 [00:12<00:00, 4632.79it/s]\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, train_data, test_data = process(X_train, Y_train, X_test,\n",
    "                                                    Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54e3cb5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:57.307001Z",
     "iopub.status.busy": "2022-08-20T18:19:57.306614Z",
     "iopub.status.idle": "2022-08-20T18:19:57.311987Z",
     "shell.execute_reply": "2022-08-20T18:19:57.310966Z"
    },
    "id": "T1b7anEMMxLy",
    "papermill": {
     "duration": 0.111654,
     "end_time": "2022-08-20T18:19:57.314012",
     "exception": false,
     "start_time": "2022-08-20T18:19:57.202358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c58779b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:57.521194Z",
     "iopub.status.busy": "2022-08-20T18:19:57.520839Z",
     "iopub.status.idle": "2022-08-20T18:19:57.533236Z",
     "shell.execute_reply": "2022-08-20T18:19:57.532375Z"
    },
    "id": "B84c8EISMxLy",
    "papermill": {
     "duration": 0.117173,
     "end_time": "2022-08-20T18:19:57.535313",
     "exception": false,
     "start_time": "2022-08-20T18:19:57.418140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SentimentNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 bidirectional,\n",
    "                 vocab_size=len(word2idx),\n",
    "                 output_size=1,\n",
    "                 embedding_dim=400,\n",
    "                 hidden_dim=512,\n",
    "                 n_layers=2):\n",
    "\n",
    "        super(SentimentNet, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if model == 'lstm':\n",
    "            self.rnn = nn.LSTM(embedding_dim,\n",
    "                               hidden_dim,\n",
    "                               n_layers,\n",
    "                               dropout=0.5,\n",
    "                               batch_first=True,\n",
    "                               bidirectional=bidirectional)\n",
    "        elif model == 'gru':\n",
    "            self.rnn = nn.GRU(embedding_dim,\n",
    "                              hidden_dim,\n",
    "                              n_layers,\n",
    "                              dropout=0.5,\n",
    "                              batch_first=True,\n",
    "                              bidirectional=bidirectional)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        out, hidden = self.rnn(embeds, hidden)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size,\n",
    "                             self.hidden_dim).zero_().to(device),\n",
    "                  weight.new(self.n_layers, batch_size,\n",
    "                             self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6cb56a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:57.760615Z",
     "iopub.status.busy": "2022-08-20T18:19:57.760219Z",
     "iopub.status.idle": "2022-08-20T18:19:57.775217Z",
     "shell.execute_reply": "2022-08-20T18:19:57.774383Z"
    },
    "id": "ohRCWdWhMxLy",
    "papermill": {
     "duration": 0.136265,
     "end_time": "2022-08-20T18:19:57.777221",
     "exception": false,
     "start_time": "2022-08-20T18:19:57.640956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(network,\n",
    "          train_loader,\n",
    "          valid_loader=None,\n",
    "          epochs=5,\n",
    "          clip=5,\n",
    "          interval=500,\n",
    "          validation_accuracy=[],\n",
    "          verbose=True):\n",
    "\n",
    "    optimiser = torch.optim.Adam(network.parameters(), lr=0.005)\n",
    "    loss_function = nn.BCELoss()\n",
    "    network.to(device)\n",
    "\n",
    "    loss_min = np.inf\n",
    "    max_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "        train_loss = []\n",
    "        _ = network.init_hidden(batch_size)\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.long().to(device), labels.to(device)\n",
    "\n",
    "            output, _ = network(inputs, None)\n",
    "            loss = loss_function(output.squeeze(-1), labels.float())\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            network.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(network.parameters(), clip)\n",
    "            optimiser.step()\n",
    "\n",
    "            if i % interval == 0 and verbose:\n",
    "                print('Epoch: {}/{}, {}%, Loss: {}'.format(\n",
    "                    epoch + 1, epochs, round(i / len(train_loader) * 100, 2),\n",
    "                    loss),\n",
    "                      end='\\r')\n",
    "        epoch_loss = np.mean(train_loss)\n",
    "        if verbose:\n",
    "            print('\\rEpoch: {}/{}, Training Loss: {}'.format(\n",
    "                epoch + 1, epochs, epoch_loss))\n",
    "\n",
    "        if valid_loader != None:\n",
    "            network.eval()\n",
    "            valid_loss = []\n",
    "            num_correct = 0\n",
    "            for i, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs, labels = inputs.long().to(device), labels.to(device)\n",
    "                output, _ = network(inputs, None)\n",
    "                loss = loss_function(output.squeeze(-1), labels.float())\n",
    "                valid_loss.append(loss.item())\n",
    "                pred = torch.round(output.squeeze(-1))\n",
    "                correct = pred.eq(labels.float()).cpu().numpy()\n",
    "                num_correct += np.sum(correct)\n",
    "\n",
    "            if verbose:\n",
    "                print('Epoch: {}/{}, Validation Loss: {}'.format(\n",
    "                    epoch + 1, epochs, np.mean(valid_loss)),\n",
    "                      end='\\r')\n",
    "            valid_acc = num_correct / len(valid_loader.dataset)\n",
    "            max_acc = max(max_acc, valid_acc)\n",
    "            if verbose:\n",
    "                print('Epoch: {}/{}, Validation Accuracy: {:.3f}%'.format(\n",
    "                    epoch + 1, epochs, valid_acc * 100),\n",
    "                      end='\\r')\n",
    "\n",
    "        if epoch_loss < loss_min:\n",
    "            torch.save(network.state_dict(), 'bestnetwork_sentiment.pt')\n",
    "            if verbose:\n",
    "                print('Best network saved\\n')\n",
    "            loss_min = epoch_loss\n",
    "\n",
    "    if valid_loader != None:\n",
    "        validation_accuracy.append(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78d715c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:58.025754Z",
     "iopub.status.busy": "2022-08-20T18:19:58.025211Z",
     "iopub.status.idle": "2022-08-20T18:19:58.032697Z",
     "shell.execute_reply": "2022-08-20T18:19:58.031728Z"
    },
    "id": "BhWvZwpeMxLz",
    "papermill": {
     "duration": 0.113439,
     "end_time": "2022-08-20T18:19:58.034809",
     "exception": false,
     "start_time": "2022-08-20T18:19:57.921370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(network, test_loader):\n",
    "    loss_function = nn.BCELoss()\n",
    "    test_loss = []\n",
    "    num_correct = 0\n",
    "    _ = network.init_hidden(batch_size)\n",
    "\n",
    "    network.eval()\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.long().to(device), labels.to(device)\n",
    "\n",
    "        output, _ = network(inputs, None)\n",
    "        loss = loss_function(output.squeeze(-1), labels.float())\n",
    "        test_loss.append(loss.item())\n",
    "        pred = torch.round(output.squeeze(-1))\n",
    "\n",
    "        correct = pred.eq(labels.float()).cpu().numpy()\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "    print('Testing loss: {:.3f}'.format(np.mean(test_loss)))\n",
    "    test_acc = num_correct / len(test_loader.dataset)\n",
    "    print('Testing accuracy: {:.3f}%'.format(test_acc * 100))\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d5b5731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:19:58.242380Z",
     "iopub.status.busy": "2022-08-20T18:19:58.241377Z",
     "iopub.status.idle": "2022-08-20T18:21:53.734603Z",
     "shell.execute_reply": "2022-08-20T18:21:53.733387Z"
    },
    "id": "MgOaVv4Jcvz3",
    "outputId": "98129d5b-54df-40e2-c96a-ca5916ac76a3",
    "papermill": {
     "duration": 115.600072,
     "end_time": "2022-08-20T18:21:53.737927",
     "exception": false,
     "start_time": "2022-08-20T18:19:58.137855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 400)\n",
      "  (rnn): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.5949169172445933\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.4922956688086192\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.45235573514302574\n",
      "Best network saved\n",
      "\n",
      "Epoch: 4/5, Training Loss: 0.4324088364283244\n",
      "Best network saved\n",
      "\n",
      "Epoch: 5/5, Training Loss: 0.42326335916519164\n",
      "Best network saved\n",
      "\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.509\n",
      "Testing accuracy: 75.209%\n"
     ]
    }
   ],
   "source": [
    "bestNetwork = {}\n",
    "network = SentimentNet(model='lstm', bidirectional=False)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "bestNetwork[('lstm', False)] = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfca1f1",
   "metadata": {
    "id": "TAvD1lHGMxLz",
    "papermill": {
     "duration": 0.102571,
     "end_time": "2022-08-20T18:21:53.945336",
     "exception": false,
     "start_time": "2022-08-20T18:21:53.842765",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2. (2 pts) Modify the model from 3.1 to use GRU. Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2627ae69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:21:54.153116Z",
     "iopub.status.busy": "2022-08-20T18:21:54.152756Z",
     "iopub.status.idle": "2022-08-20T18:23:34.420942Z",
     "shell.execute_reply": "2022-08-20T18:23:34.419593Z"
    },
    "id": "TfLDxsxoY9b3",
    "outputId": "dabec2a9-beb0-4c14-cdc9-fecc8c90f00e",
    "papermill": {
     "duration": 100.374754,
     "end_time": "2022-08-20T18:23:34.423800",
     "exception": false,
     "start_time": "2022-08-20T18:21:54.049046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 400)\n",
      "  (rnn): GRU(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.6132331674893697\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.5983517466068268\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.6192516043821971\n",
      "Epoch: 4/5, Training Loss: 0.6375154728571574\n",
      "Epoch: 5/5, Training Loss: 0.629681940428416\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.641\n",
      "Testing accuracy: 63.788%\n"
     ]
    }
   ],
   "source": [
    "network = SentimentNet(model='gru', bidirectional=False)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "bestNetwork[('gru', False)] = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5272b",
   "metadata": {
    "id": "caW4h3rjMxLz",
    "papermill": {
     "duration": 0.103782,
     "end_time": "2022-08-20T18:23:34.636069",
     "exception": false,
     "start_time": "2022-08-20T18:23:34.532287",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3. (5 pts) Modify the model from 3.1 to use bidirectional LSTM. Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "212ce91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:23:34.986923Z",
     "iopub.status.busy": "2022-08-20T18:23:34.986470Z",
     "iopub.status.idle": "2022-08-20T18:27:18.703887Z",
     "shell.execute_reply": "2022-08-20T18:27:18.702282Z"
    },
    "id": "rxL4iTP0gL7a",
    "outputId": "de9d0a97-d77c-4451-d589-bca7bd2ed7e3",
    "papermill": {
     "duration": 223.88339,
     "end_time": "2022-08-20T18:27:18.706040",
     "exception": false,
     "start_time": "2022-08-20T18:23:34.822650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 400)\n",
      "  (rnn): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.6184585534413656\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.511397004421552\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.48436869036356606\n",
      "Best network saved\n",
      "\n",
      "Epoch: 4/5, Training Loss: 0.4665720727602641\n",
      "Best network saved\n",
      "\n",
      "Epoch: 5/5, Training Loss: 0.4656448189576467\n",
      "Best network saved\n",
      "\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.611\n",
      "Testing accuracy: 73.816%\n"
     ]
    }
   ],
   "source": [
    "network = SentimentNet(model='lstm', bidirectional=True)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "bestNetwork[('lstm', True)] = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03d2361",
   "metadata": {
    "id": "Okrq9C-JMxL0",
    "papermill": {
     "duration": 0.104747,
     "end_time": "2022-08-20T18:27:18.916399",
     "exception": false,
     "start_time": "2022-08-20T18:27:18.811652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4. (2 pts) Modify the model from 3.1 to use bidirectional GRU. Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f22290e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:27:19.128516Z",
     "iopub.status.busy": "2022-08-20T18:27:19.128171Z",
     "iopub.status.idle": "2022-08-20T18:30:32.491243Z",
     "shell.execute_reply": "2022-08-20T18:30:32.489591Z"
    },
    "id": "sBaki2l7geDr",
    "outputId": "67ef3357-bb43-4dd4-d6ad-a01a4f4f13ef",
    "papermill": {
     "duration": 193.471741,
     "end_time": "2022-08-20T18:30:32.493582",
     "exception": false,
     "start_time": "2022-08-20T18:27:19.021841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 400)\n",
      "  (rnn): GRU(400, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=1024, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.6442613546848297\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.611183275492986\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.6187502720673879\n",
      "Epoch: 4/5, Training Loss: 0.652715049346288\n",
      "Epoch: 5/5, Training Loss: 0.6816603932221731\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.887\n",
      "Testing accuracy: 59.889%\n"
     ]
    }
   ],
   "source": [
    "network = SentimentNet(model='gru', bidirectional=True)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "bestNetwork[('gru', True)] = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c7b12d",
   "metadata": {
    "id": "Kk7frsSoMxL0",
    "papermill": {
     "duration": 0.147995,
     "end_time": "2022-08-20T18:30:32.748381",
     "exception": false,
     "start_time": "2022-08-20T18:30:32.600386",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5. (5 pts) Pick the best model so far and train the model starting from pretrained [GloveTwitter100d](https://nlp.stanford.edu/projects/glove/) (use the same vocabulary as before, just initialize the embedding of the words using Glove embeddings). Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a84e957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:30:33.094962Z",
     "iopub.status.busy": "2022-08-20T18:30:33.094516Z",
     "iopub.status.idle": "2022-08-20T18:36:48.954357Z",
     "shell.execute_reply": "2022-08-20T18:36:48.953229Z"
    },
    "id": "UvlYs4WM_37Y",
    "papermill": {
     "duration": 376.026968,
     "end_time": "2022-08-20T18:36:48.957444",
     "exception": false,
     "start_time": "2022-08-20T18:30:32.930476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./vectorEmbeddings/glove.twitter.27B.zip: 1.52GB [04:48, 5.27MB/s]                            \n",
      "100%|█████████▉| 1193513/1193514 [00:47<00:00, 24975.50it/s]\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 100\n",
    "Glove = torchtext.vocab.GloVe(name='twitter.27B',\n",
    "                              dim=embed_dim,\n",
    "                              cache='./vectorEmbeddings')\n",
    "\n",
    "pretrained_embed = np.zeros((len(word2idx), embed_dim))\n",
    "for i, word in enumerate(word2idx.keys()):\n",
    "    if word in Glove.stoi:\n",
    "        pretrained_embed[i] = Glove[word]\n",
    "    else:\n",
    "        pretrained_embed[i] = np.random.normal(scale=0.6, size=(embed_dim, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4772254",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:36:49.483138Z",
     "iopub.status.busy": "2022-08-20T18:36:49.482442Z",
     "iopub.status.idle": "2022-08-20T18:38:28.547217Z",
     "shell.execute_reply": "2022-08-20T18:38:28.546206Z"
    },
    "id": "XjuHIAd0MxL0",
    "outputId": "7cd9d0a5-acd4-44bb-e512-ea2dc5f1e146",
    "papermill": {
     "duration": 99.594639,
     "end_time": "2022-08-20T18:38:28.815125",
     "exception": false,
     "start_time": "2022-08-20T18:36:49.220486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBest network:\u001b[0m lstm False\n",
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 100)\n",
      "  (rnn): LSTM(100, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.6902948512713114\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.6298595042228698\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.5164655623277028\n",
      "Best network saved\n",
      "\n",
      "Epoch: 4/5, Training Loss: 0.4631766457716624\n",
      "Best network saved\n",
      "\n",
      "Epoch: 5/5, Training Loss: 0.42771753466924034\n",
      "Best network saved\n",
      "\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.526\n",
      "Testing accuracy: 76.602%\n"
     ]
    }
   ],
   "source": [
    "model, bidirectional = max(bestNetwork, key=bestNetwork.get)\n",
    "print('\\033[1mBest network:\\033[0m', model, bidirectional)\n",
    "\n",
    "network = SentimentNet(model=model,\n",
    "                       embedding_dim=embed_dim,\n",
    "                       bidirectional=bidirectional)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "network.embedding.weight.data.copy_(torch.from_numpy(pretrained_embed))\n",
    "network.embedding.weight.requires_grad = True\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "_ = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721a3a28",
   "metadata": {
    "id": "HEmGWNuAMxL2",
    "papermill": {
     "duration": 0.262899,
     "end_time": "2022-08-20T18:38:29.341971",
     "exception": false,
     "start_time": "2022-08-20T18:38:29.079072",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6. (10 pts) Using your best model so far, conduct a 5-fold (stratified) cross validation on your training data and a grid search to pick the best hidden size (try 128 or 512) and embedding size (try 100 or 400). Compute and report the average accuracies for each of the choice combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "031f667d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:38:29.864765Z",
     "iopub.status.busy": "2022-08-20T18:38:29.864383Z",
     "iopub.status.idle": "2022-08-20T18:38:29.870368Z",
     "shell.execute_reply": "2022-08-20T18:38:29.869388Z"
    },
    "id": "Rw5nrnO-MxL2",
    "papermill": {
     "duration": 0.271379,
     "end_time": "2022-08-20T18:38:29.872461",
     "exception": false,
     "start_time": "2022-08-20T18:38:29.601082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitData(dataset, ratio=0.8, batch_size=32):\n",
    "    train_size = int(ratio * len(dataset))\n",
    "    train_data, valid_data = random_split(\n",
    "        dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "    valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dc5ca63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T18:38:30.455864Z",
     "iopub.status.busy": "2022-08-20T18:38:30.455490Z",
     "iopub.status.idle": "2022-08-20T19:01:24.284739Z",
     "shell.execute_reply": "2022-08-20T19:01:24.283652Z"
    },
    "id": "1AHtJX1SgrbI",
    "outputId": "6dd21339-f098-468c-91f1-d91eb455a7e8",
    "papermill": {
     "duration": 1374.408202,
     "end_time": "2022-08-20T19:01:24.599584",
     "exception": false,
     "start_time": "2022-08-20T18:38:30.191382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mHidden Dim:\u001b[0m 128 \u001b[1mEmbedding Dim:\u001b[0m 100\n",
      "\u001b[1mAverage validation accuracy:\u001b[0m 76.168%\n",
      "\u001b[1mHidden Dim:\u001b[0m 128 \u001b[1mEmbedding Dim:\u001b[0m 400\n",
      "\u001b[1mAverage validation accuracy:\u001b[0m 75.772%\n",
      "\u001b[1mHidden Dim:\u001b[0m 512 \u001b[1mEmbedding Dim:\u001b[0m 100\n",
      "\u001b[1mAverage validation accuracy:\u001b[0m 74.240%\n",
      "\u001b[1mHidden Dim:\u001b[0m 512 \u001b[1mEmbedding Dim:\u001b[0m 400\n",
      "\u001b[1mAverage validation accuracy:\u001b[0m 75.462%\n"
     ]
    }
   ],
   "source": [
    "bestParams = {}\n",
    "for hidden_dim in [128, 512]:\n",
    "    for embedding_dim in [100, 400]:\n",
    "        print('\\033[1mHidden Dim:\\033[0m', hidden_dim,\n",
    "              '\\033[1mEmbedding Dim:\\033[0m', embedding_dim)\n",
    "\n",
    "        validation_accuracy = []\n",
    "        for k in range(5):\n",
    "            print('\\rFold: {}'.format(k + 1), end='')\n",
    "\n",
    "            train_loader, valid_loader = splitData(train_data)\n",
    "\n",
    "            network = SentimentNet(embedding_dim=embedding_dim,\n",
    "                                   hidden_dim=hidden_dim,\n",
    "                                   model=model,\n",
    "                                   bidirectional=bidirectional)\n",
    "\n",
    "            train(network=network,\n",
    "                  train_loader=train_loader,\n",
    "                  valid_loader=valid_loader,\n",
    "                  validation_accuracy=validation_accuracy,\n",
    "                  verbose=False)\n",
    "\n",
    "        average_accuracy = np.mean(validation_accuracy)\n",
    "        print('\\r\\033[1mAverage validation accuracy:\\033[0m {:.3f}%'.format(\n",
    "            average_accuracy * 100))\n",
    "        bestParams[(hidden_dim, embedding_dim)] = average_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631d33a",
   "metadata": {
    "id": "31zaS7HmMxL2",
    "papermill": {
     "duration": 0.263112,
     "end_time": "2022-08-20T19:01:25.124917",
     "exception": false,
     "start_time": "2022-08-20T19:01:24.861805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 7. (3 pts) Train the model on all your training data using the best combination of hyperparameters you find in 3.6. Compute and report the accuracy on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f0a2673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:01:25.664114Z",
     "iopub.status.busy": "2022-08-20T19:01:25.663751Z",
     "iopub.status.idle": "2022-08-20T19:01:25.676487Z",
     "shell.execute_reply": "2022-08-20T19:01:25.675572Z"
    },
    "id": "SBSa5kNSMxL2",
    "papermill": {
     "duration": 0.291053,
     "end_time": "2022-08-20T19:01:25.678636",
     "exception": false,
     "start_time": "2022-08-20T19:01:25.387583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7984ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:01:26.253345Z",
     "iopub.status.busy": "2022-08-20T19:01:26.252976Z",
     "iopub.status.idle": "2022-08-20T19:02:13.748360Z",
     "shell.execute_reply": "2022-08-20T19:02:13.746821Z"
    },
    "id": "AnyHidgoMxL2",
    "outputId": "41b9ad22-81f8-4fa3-8500-9b44d610308e",
    "papermill": {
     "duration": 47.765337,
     "end_time": "2022-08-20T19:02:13.751690",
     "exception": false,
     "start_time": "2022-08-20T19:01:25.986353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork:\u001b[0m\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(19959, 100)\n",
      "  (rnn): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/5, Training Loss: 0.6691614678064982\n",
      "Best network saved\n",
      "\n",
      "Epoch: 2/5, Training Loss: 0.5128658341169358\n",
      "Best network saved\n",
      "\n",
      "Epoch: 3/5, Training Loss: 0.4514279352426529\n",
      "Best network saved\n",
      "\n",
      "Epoch: 4/5, Training Loss: 0.415068078037103\n",
      "Best network saved\n",
      "\n",
      "Epoch: 5/5, Training Loss: 0.38777630302906035\n",
      "Best network saved\n",
      "\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.479\n",
      "Testing accuracy: 76.602%\n"
     ]
    }
   ],
   "source": [
    "hidden_dim, embedding_dim = max(bestParams, key=bestParams.get)\n",
    "\n",
    "network = SentimentNet(embedding_dim=embedding_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       model=model,\n",
    "                       bidirectional=bidirectional)\n",
    "print('\\033[1mNetwork:\\033[0m\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "_ = test(network, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8bd87",
   "metadata": {
    "id": "ib8MbN0OMxL3",
    "papermill": {
     "duration": 0.265676,
     "end_time": "2022-08-20T19:02:14.298043",
     "exception": false,
     "start_time": "2022-08-20T19:02:14.032367",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 8. (BONUS: 10 pts) Train your best model using the best hyperparameters from 3.6 on all the [sentiment140 data](http://help.sentiment140.com/for-students/). Compute and report the accuracy on the test data from HW2 (i.e., *sentiment-test.csv*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25a61f7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:02:14.876228Z",
     "iopub.status.busy": "2022-08-20T19:02:14.875851Z",
     "iopub.status.idle": "2022-08-20T19:02:20.652930Z",
     "shell.execute_reply": "2022-08-20T19:02:20.651907Z"
    },
    "id": "YLxyZlL0dSxP",
    "papermill": {
     "duration": 6.045497,
     "end_time": "2022-08-20T19:02:20.655430",
     "exception": false,
     "start_time": "2022-08-20T19:02:14.609933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + 'training.1600000.processed.noemoticon.csv',\n",
    "                 encoding='ISO-8859-1',\n",
    "                 header=None)\n",
    "df = df.drop(columns=[1, 2, 3, 4])\n",
    "df.columns = ['Label', 'Text']\n",
    "df['Label'] = df['Label'].apply(lambda label: 1 if label == 4 else 0)\n",
    "X_train, Y_train = df['Text'].to_list(), df['Label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bb109ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:02:21.198511Z",
     "iopub.status.busy": "2022-08-20T19:02:21.198113Z",
     "iopub.status.idle": "2022-08-20T19:08:33.952024Z",
     "shell.execute_reply": "2022-08-20T19:08:33.950992Z"
    },
    "id": "ZnceDDdVfZQ6",
    "outputId": "7fc6d072-9c1d-42a4-e8cc-c97446452be2",
    "papermill": {
     "duration": 373.034287,
     "end_time": "2022-08-20T19:08:33.955178",
     "exception": false,
     "start_time": "2022-08-20T19:02:20.920891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1599999/1600000 [05:53<00:00, 4532.18it/s]\n"
     ]
    }
   ],
   "source": [
    "word2idx, idx2word, train_data, test_data = process(X_train, Y_train, X_test,\n",
    "                                                    Y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128629d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:08:35.110878Z",
     "iopub.status.busy": "2022-08-20T19:08:35.110440Z",
     "iopub.status.idle": "2022-08-20T19:08:35.117080Z",
     "shell.execute_reply": "2022-08-20T19:08:35.116160Z"
    },
    "id": "9sw_3ANQMxL3",
    "papermill": {
     "duration": 0.700365,
     "end_time": "2022-08-20T19:08:35.120230",
     "exception": false,
     "start_time": "2022-08-20T19:08:34.419865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a61ac626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-20T19:08:36.208035Z",
     "iopub.status.busy": "2022-08-20T19:08:36.207683Z",
     "iopub.status.idle": "2022-08-20T19:16:25.100783Z",
     "shell.execute_reply": "2022-08-20T19:16:25.099787Z"
    },
    "id": "EGI54GZ8MxL3",
    "outputId": "2c28f636-8533-419f-8ee7-24dcadf46bca",
    "papermill": {
     "duration": 469.680336,
     "end_time": "2022-08-20T19:16:25.456611",
     "exception": false,
     "start_time": "2022-08-20T19:08:35.776275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNetwork\u001b[0m:\n",
      "SentimentNet(\n",
      "  (embedding): Embedding(247693, 100)\n",
      "  (rnn): LSTM(100, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "\n",
      "\u001b[1mTraining...\u001b[0m\n",
      "Epoch: 1/1, Training Loss: 0.477465318351686\n",
      "Best network saved\n",
      "\n",
      "\n",
      "\u001b[1mTesting...\u001b[0m\n",
      "Testing loss: 0.418\n",
      "Testing accuracy: 81.337%\n"
     ]
    }
   ],
   "source": [
    "network = SentimentNet(vocab_size=len(word2idx),\n",
    "                       embedding_dim=embedding_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       model=model,\n",
    "                       bidirectional=bidirectional)\n",
    "print('\\033[1mNetwork\\033[0m:\\n{}'.format(network))\n",
    "\n",
    "print('\\n\\033[1mTraining...\\033[0m')\n",
    "train(network=network, train_loader=train_loader, epochs=1, interval=5000)\n",
    "\n",
    "print('\\n\\033[1mTesting...\\033[0m')\n",
    "_ = test(network, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4009.264281,
   "end_time": "2022-08-20T19:16:28.267474",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-20T18:09:39.003193",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
